---
# This playbook grows the root VG on a node by:
#  * expanding /dev/nvme0n1
#  * using fdisk to expand the rootvg partition
#  * grow /var to 64GB
#
#  To run:
#  1. Source your AWS credentials (make sure it's the corresponding AWS account) into your environment
#    export AWS_PROFILE=<aws account>
#
# 2. run the playbook:
#   ansible-playbook -e 'cli_tag_name=<tag-name>' grow_root_vg-m5.yml
#
#  Example:
#   ansible-playbook -e 'cli_tag_name=ops-compute-12345' grow_root_vg-m5.yml
#
#  Notes:
#  * By default this will do a 90GB GP2 volume.  The volume size can be overidden with the "-e 'cli_volume_size=something'" variable
#  * This can be done with NO downtime on the host, but you probably ought to test a reboot. 
#  * cli_reserved_sector_count exists because for NVMe EBS volumes Host Protected Area (HPA) is enabled, but we cannot determine the number of sectors reserved with hdparm -N
#

- name: Grow the rootvg volume group
  hosts: "oo_name_{{ cli_tag_name }}"
  user: root
  connection: ssh
  gather_facts: no

  vars:
    cli_volume_size: 90
    cli_var_size: 64
    cli_reserved_sector_count: 0
    os_size_min: 20
    cli_var_partition_number: 2

  tasks:
  - name: "Check for required variables" 
    fail:
      msg: "This playbook requires {{item}} to be set."
    when: "{{ item }} is not defined or {{ item }} == ''"
    with_items:
    - cli_tag_name
    - cli_volume_size
    - cli_var_size

  - name: "Sanity check variables" 
    fail: 
      msg: "cli_volume_size should be at least cli_var_size + {{ os_size_min }}" 
    when: "{{ cli_volume_size|int - cli_var_size|int }} < os_size_min|int "

  - name: Install nvme-cli
    yum:
      name: nvme-cli
      state: installed

  # The nvme device has a mapping to an amazon device, typically "sda1"
  # openshift-tools function get_volume_id_by_linux_device does /sd/xvd/ on the AWS device name, meaning "/dev/sda1" -> "/dev/xvda1"
  # Therefore, this script converts "sda1" to "/dev/xvda1", because "sda1" is the root volume
  - name: Get aws device name (different from instance device)
    shell: > 
      for DEV_NVME in `nvme list | grep "^/dev" | awk '{print $1}'`;
      do
        if [ "$(lsblk $DEV_NVME | grep rootvg)" != "" ]; then
          nvme id-ctrl --raw-binary $DEV_NVME 2> /dev/null | cut -c3073-3104 | tr -s ' ' | sed 's#\(^[^/].*\)#/dev/\1#g' | sed 's/sda/xvda/g'
        fi
      done
    changed_when: false
    register: rootvg_aws
    ignore_errors: yes

  - name: Attempt to find the physical device that rootvg is using (NVMe)
    shell: >
      for DEV_NVME in `nvme list | grep "^/dev" | awk '{print $1}'`;
      do
        if [ "$(lsblk $DEV_NVME | grep rootvg)" != "" ]; then
          echo $DEV_NVME
          break
        fi
      done
    changed_when: false
    register: rootvg_instance
    ignore_errors: yes

  - name: fail if we don't find a single rootvg physical volume
    fail:
      msg:  Unable to find single rootvg physical volume. Please investigate manually.
    when: rootvg_instance.stdout_lines|length != 1

  - name: get list of volumes from AWS
    delegate_to: localhost
    ec2_vol:
      state: list
      instance: "{{ ec2_id }}"
      region: "{{ ec2_region }}"
    register: attached_volumes

  - debug: var=attached_volumes

  - name: set vol_instance volume attributes
    set_fact:
      vol_instance: "{{ rootvg_instance.stdout | vol_attrs }}"

  - debug: var=vol_instance

  - name: set vol_aws volume attributes
    set_fact:
      vol_aws: "{{ rootvg_aws.stdout | vol_attrs }}"

  - debug: var=vol_aws

  - name: debug attached_volumes.volumes
    debug:
      var: attached_volumes

  - name: Fail if there are no attached volumes
    fail:
      msg: Unable to find any attached volumes. Please ensure that AWS_PROFILE and AWS_DEFAULT_REGION env vars are set.
    when: attached_volumes.volumes | length < 1

  # didn't want to unwind all the use of vol_aws, but this is a way to get the volume device id directly from the instance
#  - name: get ebs volume id using smartctl
#    shell: "smartctl -i {{ vol_instance.fullname }} | grep 'Serial Number' | sed 's/.*vol\\(.*\\)/vol-\\1/g'"
#    register: rootvg_volume_id_out

  - name: get volume id of current rootvg volume
    set_fact:
      rootvg_volume_id: "{{ attached_volumes.volumes | get_volume_id_by_linux_device(vol_aws.device, true) }}"

  - debug: var=rootvg_volume_id

  - fail:
      msg: "Unable to look up the volume ID of {{ vol_aws.device }}"
    when: rootvg_volume_id == ""

  - name: get volume size of current rootvg volume
    set_fact:
      rootvg_volume_size: "{{ attached_volumes.volumes | get_volume_size_by_linux_device(vol_aws.device, true) }}"

  - debug: var=rootvg_volume_size

  - name: Take snapshot
    delegate_to: localhost
    ec2_snapshot:
      region: "{{ ec2_region }}"
      volume_id: "{{ rootvg_volume_id }}"
      description: "Snapshot of {{ cli_tag_name }} {{ vol_instance.device }} before resize"
      wait: no ## don't wait for the snapshot to complete pushing to S3, it still exists and waiting can take hours
    when: cli_volume_size|int > rootvg_volume_size|int
    register: ebs_snapshot_out

  - name: Get partition info
    parted: 
      device: "{{ vol_instance.fullname }}"
      number: "{{ cli_var_partition_number }}"
      state: info
      unit: s
    register: parted_info

  - name: set target ebs volume size (add in reserved sector size)
    set_fact:
      ebs_volume_size: "{{ ( cli_volume_size|int + ( parted_info.disk.physical_block|int * cli_reserved_sector_count|int ) / 1024 / 1024 ) | round(0, 'ceil') | int }}"

  - name: Resize EBS volume 
    delegate_to: localhost
    ## afaict the ansible ec2 modules don't offer resize
    command: "aws ec2 modify-volume --volume-id {{ rootvg_volume_id }} --size {{ ebs_volume_size }} --region {{ ec2_region }}"
    when: cli_volume_size|int > rootvg_volume_size|int
    register: resize_volume

  - debug: var=resize_volume

  - wait_for: 
      timeout: 10 
    when: cli_volume_size|int > rootvg_volume_size|int

  - name: Fail when problems extending
    fail:
      msg: "Failed to resize volume msg: {{ resize_volume.msg }}"
    when: resize_volume.msg is defined

  - name: Run partprobe to see if the size changed
    command: partprobe

  - name: Get partition info (updated)
    parted: 
      device: "{{ vol_instance.fullname }}"
      number: "{{ cli_var_partition_number }}"
      state: info
      unit: s
    register: parted_info

  - name: Get available sectors (minus reserved sector count)
    shell: "echo $(( $(fdisk -l {{ vol_instance.fullname }} | grep sectors$ | cut -d' ' -f7) - 1 - {{ cli_reserved_sector_count|int }} ))"
    changed_when: false
    register: sectors_available

  - name: Get used sectors
    shell: "fdisk -l {{ vol_instance.fullname }}| grep {{ vol_instance.fullname }} | tail -n1 | awk '{print $3}'"
    changed_when: false
    register: sectors_used

  - name: Resize partition
    when: sectors_available.stdout|int > sectors_used.stdout|int
    block:
      - name: set params for partition script
        set_fact:
          partition_name: "{{ vol_instance.fullname }}"
          partition_begin: "{{ parted_info.partitions[1].begin|int }}"
          partition_end: "{{ (parted_info.disk.size|int - cli_reserved_sector_count|int - 1) }}"
          partition_number: "{{ cli_var_partition_number }}"

      - name: Take snapshot (if didn't already)
        delegate_to: localhost
        ec2_snapshot:
          region: "{{ ec2_region }}"
          volume_id: "{{ rootvg_volume_id }}"
          description: "Snapshot of {{ cli_tag_name }} {{ vol_instance.device }} before resize"
          wait: no ## don't wait for the snapshot to complete pushing to S3, it still exists and waiting can take hours
        when: ebs_snapshot_out is not defined
        register: ebs_snapshot_out

      - name: Create script to recreate partition
        template:
          src: grow_root_vg-m5-recreate-partition.sh.j2
          dest: /root/grow_root_vg-m5-recreate-partition.sh
          mode: 0700

      - name: Recreate partition
        shell: /root/grow_root_vg-m5-recreate-partition.sh
        ignore_errors: yes

      - name: Run partprobe
        command: partprobe

  - name: Resize the pv 
    shell: "pvresize $(pvs|grep rootvg|awk '{print $1}')"

  - name: Extend the var lv
    command: "lvextend -L {{ cli_var_size }}G /dev/rootvg/var"
    ignore_errors: yes

  - name: Resize /var
    command: "xfs_growfs /var"

  - name: New /var size
    shell: "lvs rootvg | grep '^[ ]*var' | awk '{print $4}' | cut -d. -f1"
    changed_when: false
    register: new_var_size

  - name: Fail when var is not bigger than or equal to target size
    fail:
      msg: "New /var size ({{ new_var_size.stdout }}G) isn't expected size ({{ cli_var_size }}G)"
    when: new_var_size.stdout|int < cli_var_size|int

  - name: Remove recreate script when successful
    file:
      path: /root/grow_root_vg-m5-recreate-partition.sh
      state: absent

  - name: Trigger filesystem metrics (master or infra)
    shell: "docker exec -ti oso-rhel7-host-monitoring /usr/bin/cron-send-filesystem-metrics -v"
    when: "('oo_hosttype_master' in group_names or 'oo_subhosttype_infra' in group_names)"

  - name: Trigger filesystem metrics (compute)
    shell: "docker exec -ti oso-rhel7-host-monitoring /usr/bin/cron-send-filesystem-metrics --filter-pod-pv -v"
    when: "'oo_subhosttype_compute' in group_names"

